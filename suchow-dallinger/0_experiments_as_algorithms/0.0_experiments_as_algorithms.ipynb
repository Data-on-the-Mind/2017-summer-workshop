{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments as algorithms\n",
        "\n",
        "The methods that behavioral and social scientists use to answer questions about people and the societies they form have hardly changed over the past century, often drawing on experimental studies in brick & mortar labs, field observations, and some kind of modeling either through proof or simulation.\n",
        "\nThe experimental designs used in Wilhelm Wundt's lab from the early days of experimental psychology, for example, will hardly seem foreign to a modern psychologist. For example, to test a hypothesis, an experimenter may randomly assign participants to groups (e.g., treatment or control), manipulate some factor for one group but not the other, and observe whether the participants behave differently as a result. Or, to understand a functional relationship between two variables, an experimenter may ask each participant to judge a set of stimuli that vary along one variable and observe the impact on the other. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Wundt's lab](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Wundt-research-group.jpg/350px-Wundt-research-group.jpg \"Wundt's lab\")"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some ways, these designs result from constraints on how experiments are typically conducted. Human participants often visit the laboratory for a predetermined amount of time (e.g., an hour) and so it makes sense to have each participant perform a time-consuming task or respond to a number of stimuli. And because experiments are run manually by the experimenter, reconfiguring more than the group assignment or changing the set of stimuli seen by the participant can be challenging.\n",
        "\n",
        "Recently, behavioral and social scientists have begun to move from brick-and-mortar laboratories to the web, where participants are recruited through crowdsourcing services such as Amazon Mechanical Turk and use browsers to interact with web applications hosted on a server. However, the experiment designs that are used in these virtual laboratories are still largely those developed in the context of physical laboratories – crowdsourcing supports larger and more diverse samples, rapid prototyping, and high-throughput, but has not changed the fundamental structure of experiments.\n",
        "\n",
        "This is a missed opportunity. Crowdsourced experiments differ from traditional experiments in two important ways. First, participants can be recruited to complete short tasks consisting of as little as a single judgment. Second, the tasks that people are asked to perform can depend on the responses\n",
        "of previous participants. Because experiments are orchestrated by a computer rather than by the experimenter, there is more room for customizing the task that each participant performs. Combining these two properties, a crowdsourced experiment can be understood as a complex, computationally mediated adaptive iterative procedure for gathering data: an algorithm.\n",
        "\nBy taking this perspective, it becomes possible to ask what the best algorithms are for learning about human cognition and behavior. Experiment design becomes algorithm design."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\nIn this tutorial, we'll begin by reviewing several experiment designs that can be thought of as algorithms. Next, we'll talk about Dallinger, a laboratory automation system for the behavioral and social sciences, and how it can be used for algorithmic experimentation. And finally, we'll talk about reproducibility of behavioral and social science experiments and its relationship to data handling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further reading\n",
        "\n",
        "[https://github.com/suchow/awesome-crowdsourcing](https://github.com/suchow/awesome-crowdsourcing)\n",
        "\n",
        "Quinn, A. J., & Bederson, B. B. (2011, May). Human computation: a survey and taxonomy of a growing field. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 1403-1412). ACM.\n",
        "\nSuchow, J. W. & Griffiths, T. L. (2016). Rethinking experiment design as algorithm design. *CrowdML* — NIPs 2016 Workshop on Crowdsourcing and Machine Learning"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "language": "python",
      "display_name": "Python 2"
    },
    "kernel_info": {
      "name": "python2"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}