{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping with Beautiful Soup\n",
    "*****\n",
    "In this lesson we'll learn about various techniques to scrape data from websites. This lesson will include:\n",
    "\n",
    "0. Discussion of complying with Terms of Use\n",
    "1. Using Python's `BeautifulSoup` library\n",
    "2. Collecting data from one page\n",
    "3. Following collected links\n",
    "4. Exporting data to CSV\n",
    "\n",
    "\n",
    "# 0. Terms of Use\n",
    "We'll be scraping [information on the state senators of Illinois](http://www.ilga.gov/senate), as well as the [list of bills](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True) from the Illinois General Assembly. Your first step before scraping should always be to read the Terms of Use or Terms of Agreement for a website. Many websites will explicitly prohibit scraping in any form. Moreover, if you're affiliated with an institution, you may be breaching existing contracts by engaging in scraping. UC Berkeley's Library [recommends](http://guides.lib.berkeley.edu/text-mining) following this workflow:\n",
    "\n",
    "![UCB-library-workflow](../img/UCB-library-workflow.png)\n",
    "\n",
    "While our source's [Terms of Use](http://www.ilga.gov/disclaimer.asp) do not explicitly prohibit scraping (nor do their [robots.txt](http://www.ilga.gov/robots.txt)), it is advisable to still contact the web administrator of the website. We will not be placing too much stress on their servers today, so please keep this in mind while following along and executing the code. You should always attempt to contact the web administrator of the site you plan to scrape. Oftentimes there is an easier way to get the data that you want.\n",
    "\n",
    "Let's go ahead and import the Python libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests  # to make GET request\n",
    "from bs4 import BeautifulSoup  # to parse the HTML response\n",
    "import time  # to pause between calls\n",
    "import csv  # to write data to csv\n",
    "import pandas  # to see CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using `BeautifulSoup`\n",
    "*****\n",
    "\n",
    "## 1.1 Make a GET request and parse the HTML response\n",
    "\n",
    "We use the `requests` library just as we did with APIs, but this time we won't get JSON or XML back, but we'll get an HTML response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "response = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "\n",
    "# read the content of the server’s response as a string\n",
    "page_source = response.text\n",
    "print(page_source[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 *soup* it\n",
    "\n",
    "Now we use the `BeautifulSoup` function to make an object of the response, which allows us to parse the HTML tree. This returns an object (called a *soup* object) with all of the HTML in the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse the response into an HTML tree soup object\n",
    "soup = BeautifulSoup(page_source, 'html5lib')\n",
    "\n",
    "# take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Find Elements\n",
    "\n",
    "`BeautifulSoup` has a number of functions to find things on a page. Like other scraping tools, `BeautifulSoup` lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: Because `find_all()` is the most popular method in the `BeautifulSoup` search library, you can use a shortcut for it. If you treat the `BeautifulSoup` object as though it were a function, then it’s the same as calling `find_all()` on that object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! Many elements on a page will have the same HTML tag. For instance, if you search for everything with the `a` tag, you're likely to get a lot of stuff, much of which you don't want. What if we wanted to search for HTML tags ONLY with certain attributes, like particular CSS classes? \n",
    "\n",
    "We can do this by adding an additional argument to the `find_all`. In the example below, we are finding all the `a` tags, and then filtering those with `class = \"sidemenu\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get only the 'a' tags in 'sidemenu' class\n",
    "soup(\"a\", class_=\"sidemenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes a more efficient way to search and find things on a website is by **CSS selector.** For this we have to use a different method, `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use \"a.sidemenu\" as a CSS selector, which returns all `a` tags with class `sidemenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get elements with \"a.sidemenu\" CSS Selector.\n",
    "soup.select(\"a.sidemenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CSS is one way to organize how we stylize a website. They allow us to categorize and label certain HTML elements, and use these categories and labels to apply specfic styling. CSS selectors are what we use to identify these elements, and then decide what style to apply. We won't have time today to go into detail about HTML and CSS, but it's worth talking about the three most important CSS selectors:\n",
    "\n",
    "1. **element selector**: simply including the element type, such as `a` above, will select all elements on the page of that element type. Try using your development tools (Chrome, Firefox, or Safari) to change all elements of the type `a` to a background color of `red`.\n",
    "\n",
    "```\n",
    "a {\n",
    "    background-color: red\n",
    "}\n",
    "```\n",
    "\n",
    "2. **class selector**: if you put a period (`.`) before the name of a class, all elements belonging to that class will be selected. Try using your development tools to change all elements of the class `detail` to a background color of `red`.\n",
    "\n",
    "```\n",
    ".detail {\n",
    "    background-color: red\n",
    "}\n",
    "```\n",
    "\n",
    "3. **ID selector**: if you put a hashtag (`#`) before the name of an id, all elements with that id will be selected. Try using the development tools to change all elements with the id `Senate` to a background color of `red`.\n",
    "\n",
    "```\n",
    "#Senate {\n",
    "    background-color: red\n",
    "}\n",
    "```\n",
    "\n",
    "The above three examples will take all elements with the given property, but oftentimes you only want certain elements within the hierarchy. We can do that by simply placing elements side-by-side separated by a space.\n",
    "\n",
    "### Challenge 1\n",
    "\n",
    "Using your developer tools, change the `background-color` of all `a` elements in *only* the \"Current Senate Members\" table. \n",
    "\n",
    "```\n",
    "tr tr tr a {\n",
    "    background-color: red\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "Find all the `<a>` elements in class `mainmenu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "soup.select(\"a.mainmenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Get Attributes and Text of Elements\n",
    "\n",
    "Once we identify elements, we want to access information in that element. Oftentimes this means two things:\n",
    "\n",
    "1. Text\n",
    "2. Attributes\n",
    "\n",
    "Getting the text inside an element is easy. All we have to do is use the `text` member of a `tag` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a list\n",
    "soup.select(\"a.sidemenu\")\n",
    "\n",
    "# we first want to get an individual tag object\n",
    "first_link = soup.select(\"a.sidemenu\")[0]\n",
    "\n",
    "# check out its class\n",
    "print(type(first_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tag! Which means it has a `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see there is some extra spacing here, we can use the `strip` method to remove that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(first_link.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want the value of certain attributes. This is particularly relevant for `a` tags, or links, where the `href` attribute tells us where the link goes.\n",
    "\n",
    "You can access a tag’s attributes by treating the tag like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, but that doesn't look like a full URL! Don't worry, we'll get to this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3\n",
    "\n",
    "Find all the `href` attributes (url) from the mainmenu by writing a list comprehension and assign to it `rel_paths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "rel_paths = [link['href'] for link in soup.select(\"a.mainmenu\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rel_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collecting information\n",
    "*****\n",
    "\n",
    "Believe it or not, that's all you need to scrape a website. Let's apply these skills to scrape the [98th general assembly](http://www.ilga.gov/senate/default.asp?GA=98).\n",
    "\n",
    "Our goal is to scrape information on each senator, including their:\n",
    "* name\n",
    "* district\n",
    "* party\n",
    "\n",
    "## 2.1 First, make the GET request and *soup* it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "response = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "page_source = response.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(page_source, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Find the right elements and text\n",
    "\n",
    "Now let's try to get a list of rows in that table. Remember that rows are identified by the `tr` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all tr elements\n",
    "rows = soup.find_all(\"tr\")\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember, `find_all` gets all the elements with the `tr` tag. We can use smart CSS selectors to get only the rows we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "print(rows[2].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `select` method on anything. Let's say we want to find everything with the CSS selector `td.detail` in an item of the list we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select only those 'td' tags with class 'detail'\n",
    "row = rows[2]\n",
    "detail_cells = row.select('td.detail')\n",
    "detail_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we're interested in the actual **text** of a website, not its tags. Remember, to get the text of an HTML element, use the `text` member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only the text in each of those cells\n",
    "row_data = [cell.text for cell in detail_cells]\n",
    "print(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the `BeautifulSoup` tools with our basic python skills to scrape an entire web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check it out\n",
    "print(row_data[0]) # name\n",
    "print(row_data[3]) # district\n",
    "print(row_data[4]) # party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Loop it all together\n",
    "\n",
    "### Challenge 4\n",
    "\n",
    "Let's use a `for` loop to get 'em all! We'll start at the beginning with the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "response = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "page_source = response.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(page_source, \"html5lib\")\n",
    "\n",
    "# create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# loop through all rows\n",
    "for row in rows:\n",
    "\n",
    "    # select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail')\n",
    "    \n",
    "    # get rid of junk rows\n",
    "    if len(detail_cells) is not 5: \n",
    "        continue\n",
    "        \n",
    "    # keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    \n",
    "    # collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "    \n",
    "    # store in a tuple\n",
    "    tup = (name, district, party)\n",
    "    \n",
    "    # append to list\n",
    "    members.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(members))\n",
    "print()\n",
    "print(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Get HREF element pointing to members' bills\n",
    "\n",
    "The code above retrieves information on:  \n",
    "\n",
    "* the senator's name\n",
    "* their district number\n",
    "* and their party\n",
    "\n",
    "We now want to retrieve the URL for each senator's list of bills. The format for the list of bills for a given senator is:\n",
    "\n",
    "http://www.ilga.gov/senate/SenatorBills.asp + ? + GA=98 + &MemberID=**_memberID_** + &Primary=True\n",
    "\n",
    "to get something like:\n",
    "\n",
    "http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True\n",
    "\n",
    "You should be able to see that, unfortunately, _memberID_ is not currently something pulled out in our scraping code.\n",
    "\n",
    "Your initial task is to modify the code above so that we also **retrieve the full URL which points to the corresponding page of primary-sponsored bills**, for each member, and return it along with their name, district, and party.\n",
    "\n",
    "Tips: \n",
    "\n",
    "* To do this, you will want to get the appropriate anchor element (`<a>`) in each legislator's row of the table. You can again use the `.select()` method on the `row` object in the loop to do this — similar to the command that finds all of the `td.detail` cells in the row. Remember that we only want the link to the legislator's bills, not the committees or the legislator's profile page.\n",
    "* The anchor elements' HTML will look like `<a href=\"/senate/Senator.asp/...\">Bills</a>`. The string in the `href` attribute contains the **relative** link we are after. You can access an attribute of a `BeatifulSoup` `Tag` object the same way you access a Python dictionary: `anchor['attributeName']`. (See the <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> for more details). There are a _lot_ of different ways to use BeautifulSoup to get things done; whatever you need to do to pull that `href` out is fine.\n",
    "* Since we will only get a relative link, you'll have to do some concatenating to get the full URLs.\n",
    "\n",
    "\n",
    "**Use the code you wrote in Challenge 4 and simply add the full path to the tuple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# make a GET request\n",
    "response = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "page_source = response.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(page_source, \"html5lib\")\n",
    "\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# loop through all rows\n",
    "for row in rows:\n",
    "    # select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail')\n",
    "    \n",
    "    # get rid of junk rows\n",
    "    if len(detail_cells) is not 5: \n",
    "        continue\n",
    "        \n",
    "    # keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    \n",
    "    # collect information\n",
    "    name, district, party = row_data[0], int(row_data[3]), row_data[4]\n",
    "    \n",
    "    # add href\n",
    "    href = row.select('a')[1]['href']\n",
    "    \n",
    "    # add full path\n",
    "    full_path = \"http://www.ilga.gov/senate/\" + href + \"&Primary=True\"\n",
    "    \n",
    "    # store in a tuple\n",
    "    tup = (name, district, party, full_path)\n",
    "    \n",
    "    # append to list\n",
    "    members.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now you can probably guess how to loop it all together by iterating through the links we just extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Following links to scrape bills\n",
    "*****\n",
    "\n",
    "## 3.1 Writing a scraper function\n",
    "\n",
    "Now we want to scrape the webpages corresponding to bills sponsored by each senator.\n",
    "\n",
    "### Challenge 6\n",
    "\n",
    "Write a function called `get_bills(url)` to parse a given bill's URL. This will involve:\n",
    "\n",
    "  - requesting the URL using the <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a> library\n",
    "  - using the features of the `BeautifulSoup` library to find all of the `<td>` elements with the class `billlist`\n",
    "  - return a `list` of tuples, each with:\n",
    "      - description (2nd column)\n",
    "      - chamber (S or H) (3rd column)\n",
    "      - the last action (4th column)\n",
    "      - the last action date (5th column)\n",
    "      \n",
    "I've started the function for you. Fill in the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def get_bills(url):\n",
    "    \n",
    "    # make the GET request\n",
    "    response = requests.get(url)\n",
    "    page_source = response.text\n",
    "    soup = BeautifulSoup(page_source, \"html5lib\")\n",
    "    \n",
    "    # get the table rows\n",
    "    rows = soup.select('tr tr tr')\n",
    "    \n",
    "    # make empty list to collect the info\n",
    "    bills = []\n",
    "    for row in rows:\n",
    "        \n",
    "        # get columns\n",
    "        detail_cells = row.select('td.billlist')\n",
    "        if len(detail_cells) is not 5:\n",
    "            continue\n",
    "            \n",
    "        # get text in each column\n",
    "        row_data = [cell.text for cell in row]\n",
    "\n",
    "        # append data in columns 2-5\n",
    "        bills.append(tuple(row_data[2:6]))\n",
    "        \n",
    "    return(bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment to test your code:\n",
    "test_url = members[0][3]\n",
    "print(test_url)\n",
    "get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get all the bills\n",
    "\n",
    "Finally, we create a dictionary `bills_dict` which maps a district number (the key) onto a list_of_bills (the value) eminating from that district. You can do this by looping over all of the senate members in `members_dict` and calling `get_bills()` for each of their associated bill URLs.\n",
    "\n",
    "NOTE: Please call the function `time.sleep(5)` for each iteration of the loop, so that we don't destroy the state's web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bills_info = []\n",
    "for member in members[:3]:  # only go through 3 members\n",
    "    \n",
    "    print(member[0])\n",
    "    member_bills = get_bills(member[3])\n",
    "    for b in member_bills:\n",
    "        bill = list(member) + list(b)\n",
    "        bills_info.append(bill)\n",
    "\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bills_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Export to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can write this to a CSV too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manually decide on header names\n",
    "header = ['Senator', 'District', 'Party', 'Bills Link', 'Description', 'Chamber', 'Last Action', 'Last Action Date']\n",
    "\n",
    "with open('all-bills.csv', 'w') as output_file:\n",
    "    csv_writer = csv.writer(output_file)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(bills_info)\n",
    "    \n",
    "pandas.read_csv('all-bills.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
