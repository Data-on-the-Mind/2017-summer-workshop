{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search overview\n",
    "\n",
    "We'll end up looking at two main kinds of searches:\n",
    "* *GET search/tweets*, which returns tweets matching a search term\n",
    "* *GET statuses/user_timeline*, which returns all of a given user's tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on timeline iteration: https://dev.twitter.com/rest/public/timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET search/tweets\n",
    "\n",
    "First things first; we'll set up a search object like last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "from pprint import pprint                           #Important for reading through JSONs\n",
    "from time import localtime,strftime,sleep,time      #Important for dealing with Twitter rate limits\n",
    "import datetime                       #Important for processing Twitter timestamps\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons_oauth_file = 'c.xxx'\n",
    "if os.path.exists(cons_oauth_file):\n",
    "    constoken, conssecret = twitter.read_token_file(cons_oauth_file)\n",
    "else:\n",
    "    constoken = raw_input(\"What is your app's 'Consumer Key'?\").strip()\n",
    "    conssecret = raw_input(\"What is your app's 'Consumer Secret'?\").strip()\n",
    "    wf = open(cons_oauth_file,'w'); wf.write(constoken+'\\n'+conssecret); wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_oauth_file = 'a.xxx'\n",
    "if not os.path.exists(app_oauth_file):\t\t\t\t\t\t\t\t\t#if user not authorized already\n",
    "\ttwitter.oauth_dance(\"your app\",constoken,conssecret,app_oauth_file)\t\t#perform OAuth Dance\n",
    "apptoken, appsecret = twitter.read_token_file(app_oauth_file)\t\t\t\t\t#import user credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsearch = twitter.Twitter(auth=twitter.OAuth(apptoken,appsecret,constoken,conssecret))\t#create search command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET search/tweets\n",
    "\n",
    "Now that _tsearch_ is initialized, let's get to searching! [Here](https://dev.twitter.com/rest/reference/get/search/tweets) is Twitter's documentation on the GET search/tweets call, which is pretty good.\n",
    "\n",
    "GET search/tweets takes a range of arguments, but I find these the most important:\n",
    "* *q* : the search term, which must be UTF-8 & URL-encoded\n",
    "* *count* : how many tweets per search? (100 max)\n",
    "* *result_type* : do you want all recent tweets, or those that Twitter thinks are most interesting? (Hint: the former, you definitely want the former.)\n",
    "* *max_id* : limits results to tweets before specified tweet ID\n",
    "\n",
    "Try out a simple search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term=\"placeholder+text\"  #Note: use + instead of spaces\n",
    "\n",
    "res = tsearch.search.tweets(q=term)\n",
    "#                            count=10,              #just want 10 hits back\n",
    "#                            result_type=\"recent\")  #include all recent tweets, not only popular ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of a search is again a nested dictionary. (The Twitter API returns data in either JSON or XML format, which the *twitter* library auto-encodes as nested dictionaries.)  The returned tweets are in the 'statuses' dictionary. (Internally, the API refers to tweets as statuses, which is weird sometimes.)\n",
    "\n",
    "Let's look at the first hit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(res['statuses'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boy, that's a lot of information. Twitter calls this a fully-hydrated result, and includes information about the tweet, the user, and the social engagement of the tweet.  Here's Twitter's [overview of the information](https://dev.twitter.com/overview/api/tweets) in a tweet.\n",
    "\n",
    "I like to compare this against what Twitter shows us on its website, where the visualization is easier.  Let's make a real quick function to extract the URL from this information so we can visualize the tweets as we talk about them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracttweetURL(j):\n",
    "\treturn 'http://twitter.com/'+j['user']['screen_name']+'/status/'+str(j['id'])\n",
    "\n",
    "t = res['statuses'][0]\n",
    "print extracttweetURL(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key pieces of information depend on your goals, but in most cases these will be important:\n",
    "* *created_at*: tweet's time, in UTC.\n",
    "* *favorite_count*, *retweet_count*: number of favs & RTs, respectively, the tweet has amassed\n",
    "* *id*: tweet's unique numerical ID\n",
    "* *in_reply_to_status_id*: ID of the tweet this one's replying to (if any)\n",
    "* *text*: the text of the tweet\n",
    "* *user*: all the info about the tweeter\n",
    "\n",
    "And within the user dictionary, here're some important fields:\n",
    "* *id*: tweeter's numerical ID (constant throughout account's lifespan)\n",
    "* *location*: self-reported location of tweeter\n",
    "* *friends_count*, *follower_count*: number of people the tweeter follows and is followed by (respectively)\n",
    "* *name*: tweeter's display name (can change)\n",
    "* *screen_name*: tweeter's Twitter handle (i.e., @whatever; also can change)\n",
    "\n",
    "Just to make the tweets a little more readable, I'm going to create a pruning function down to just these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prunetweet(t):\n",
    "    d = {k: t[k] for k in ['created_at','favorite_count','retweet_count','id','in_reply_to_status_id','text']}    #keeping only relevant top-level features (user features handled below)\n",
    "    d['user'] = {k: t['user'][k] for k in ['id','location','friends_count','followers_count','name','screen_name']} #keeping only relevant features\n",
    "    return d\n",
    "\n",
    "pprint(prunetweet(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with tweets\n",
    "\n",
    "Let's play around with these a bit; try some different searches and look at the results you get. Are there any really surprising results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term=\"%22good+morning%22\"  #Note: use + instead of spaces, and %22 instead of quotes\n",
    "count=25                   #Don't bother with too many hits yet\n",
    "\n",
    "res = tsearch.search.tweets(q=term,count=count,result_type='recent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(res['statuses'])):\n",
    "    print '\\n',i, extracttweetURL(res['statuses'][i])\n",
    "    pprint(prunetweet(res['statuses'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things I've found strange/interesting/annoying:\n",
    "* a query can be matched by a username in addition to the text itself.\n",
    "* Jupyter isn't displaying Unicode well (so no emoji, :( )\n",
    "* favorite_count in tweet, favo**u**rites_count in user\n",
    "* manual RTs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating back in time\n",
    "\n",
    "Twitter limits the number of tweets from any single *GET search/tweets* call to 100. But you're allowed to go back up to one week, or 3000 tweets, whichever you run afoul of first.  How do you do that?  The result of each API call has a *search_metadata* feature, which both gives info about the completeed search and where to go from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['search_metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search API lets you specify a maximum tweet ID in each search (*max_id*), and by iteratively moving that maximum back to the minimum ID of the preceding search, you keep return results further back in time, until Twitter stops you.\n",
    "\n",
    "Unfortunately, that minimum ID is not supplied directly here; you have to extract it from the *next_results* string, or extract it manually as the minimum ID in your results.  Also, note that the *max_id* search value is **inclusive**, so you should subtract one from it before your search or you'll get that tweet over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minid = 9999999999999999999999\n",
    "for i in range(0,len(res['statuses'])):\n",
    "    #print res['statuses'][i]['id']\n",
    "    if res['statuses'][i]['id'] < minid:\n",
    "        minid = res['statuses'][i]['id']-1\n",
    "print minid\n",
    "\n",
    "#code to regexp to the max_id value and extract it as element 1 of the match object\n",
    "#see https://docs.python.org/2.7/library/re.html\n",
    "#However, Twitter is dumb if you're doing a more complex search and can't handle this, so you have to manually obtain the max_id.\n",
    "minid = re.search('max_id=([^&]+)&',res['search_metadata']['next_results']).group(1)\n",
    "print minid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = tsearch.search.tweets(q=term,count=count,result_type='recent',max_id=minid)\n",
    "res['search_metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta da!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good morning!\n",
    "\n",
    "Hey, here's a stupid test case to make sure that we're getting reasonable results. When do people say good morning?  And does it depend on where they are?\n",
    "\n",
    "Let's compare the distribution of \"good morning\" tweets in Berkeley and Pittsburgh, 3 hours apart by time zone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To go back 3000 tweets, you need 30 searches at the default count of 100 tweets per search. \n",
    "#Find out if you have enough searches left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tsearch.application.rate_limit_status()\n",
    "remaining = r['resources']['search']['/search/tweets']['remaining']\n",
    "print remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's find the 3000 most recent \"good morning\" tweets\n",
    "#Build a search for \"good morning\" (remember to convert quotes to the URL-encoded %22) & check it works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term = '%22good+morning%22'\n",
    "res = tsearch.search.tweets(q=term,count=100,result_type='recent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['search_metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract the minimum tweet ID, and get the 100 tweets before it\n",
    "#Loop 30 times (or until res['search_metadata']['count']==0)\n",
    "#Be sure to save all the tweets\n",
    "#Note: searching with max_id=0 is the same as having no max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allres = []\n",
    "minid = 9999999999999999999999\n",
    "for i in range(0,30):\n",
    "    print 'Up to tweet', minid, 'iteration', i\n",
    "    res = tsearch.search.tweets(q=term,count=100,result_type='recent',max_id=minid,geocode='37.87,-122.27,50km')\n",
    "    print 'Hits:', res['search_metadata']['count']\n",
    "    if res['search_metadata']['count']==0:\n",
    "        break\n",
    "    #minid = re.search('max_id=([^&]+)&',res['search_metadata']['next_results']).group(1)\n",
    "    for j in range(0,len(res['statuses'])):\n",
    "        #print res['statuses'][j]['id']\n",
    "        if res['statuses'][j]['id'] < minid:\n",
    "            minid = res['statuses'][j]['id']-1\n",
    "    print 'New minimum ID:', minid\n",
    "    allres.extend(res['statuses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many tweets you ended up with\n",
    "#Now let's extract their times\n",
    "\n",
    "def extracttime(t):\n",
    "    UTCoffset = datetime.timedelta(hours=7)   #I'm assuming we're on Pacific Daylight Time (UTC-7)\n",
    "    return datetime.datetime.strptime(t['created_at'],'%a %b %d %H:%M:%S +0000 %Y')-UTCoffset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the time is getting correctly calculated by comparing against web client's time.\n",
    "t=allres[0]\n",
    "print datetime.datetime.strptime(t['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "print extracttime(t)\n",
    "print extracttweetURL(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract all times\n",
    "alltimes = []\n",
    "for t in allres:\n",
    "    ti = extracttime(t)\n",
    "    alltimes.append([ti.day,ti.hour,ti.minute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the times into an array so we can easily plot them.\n",
    "import numpy as np\n",
    "timearr = np.array(alltimes)\n",
    "print timearr[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Berkeley good mornings\")\n",
    "plt.hist(timearr[:,1],range(0,25))\n",
    "plt.xlabel(\"Hour, Pacific Time\")\n",
    "plt.ylabel(\"Num. of Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People near Berkeley start saying \"good morning\" in earnest around 7am, although there's a non-negligible baseline rate at all times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now repeating for people in Pittsburgh\n",
    "allres_b = allres\n",
    "allres = []\n",
    "minid = 9999999999999999999999\n",
    "for i in range(0,30):\n",
    "    print 'Up to tweet', minid, 'iteration', i\n",
    "    res = tsearch.search.tweets(q=term,count=100,result_type='recent',max_id=minid,geocode='40.44,-80.00,50km')\n",
    "    print 'Hits:', res['search_metadata']['count']\n",
    "    if res['search_metadata']['count']==0:\n",
    "        break\n",
    "    #minid = re.search('max_id=([^&]+)&',res['search_metadata']['next_results']).group(1)\n",
    "    for j in range(0,len(res['statuses'])):\n",
    "        #print res['statuses'][j]['id']\n",
    "        if res['statuses'][j]['id'] < minid:\n",
    "            minid = res['statuses'][j]['id']-1\n",
    "    print 'New minimum ID:', minid\n",
    "    allres.extend(res['statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltimes = []\n",
    "for t in allres:\n",
    "    ti = extracttime(t)\n",
    "    alltimes.append([ti.day,ti.hour,ti.minute])\n",
    "    \n",
    "timearr = np.array(alltimes)\n",
    "print timearr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the histogram of the data\n",
    "np.histogram(timearr[:,1], 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Pittsburgh good mornings\")\n",
    "plt.hist(timearr[:,1],range(0,25))\n",
    "plt.xlabel(\"Hour, Pacific Time\")\n",
    "plt.ylabel(\"Num. of Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions?  And where from here?\n",
    "\n",
    "Obviously, this is a pretty toy-ish example, but maybe it raises some interesting ideas about what sources of noise there are. Why are there \"good morning\" tweets at the wrong times?  Is Twitter handling locations or times incorrectly, are people lying about their self-reported locations, or what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second example\n",
    "\n",
    "How do we speak about gender?\n",
    "\n",
    "Searching for *women can be* and *men can be*, and looking at the concordances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Women searches\n",
    "\n",
    "allwomen = []\n",
    "term = '%22women+can+be%22'\n",
    "minid = 9999999999999999999999\n",
    "for i in range(0,30):\n",
    "    print 'Up to tweet', minid, 'iteration', i\n",
    "    res = tsearch.search.tweets(q=term+'-rt',count=100,result_type='recent',max_id=minid)\n",
    "    print 'Hits:', res['search_metadata']['count']\n",
    "    if res['search_metadata']['count']==0:\n",
    "        break\n",
    "    #minid = re.search('max_id=([^&]+)&',res['search_metadata']['next_results']).group(1)\n",
    "    for j in range(0,len(res['statuses'])):\n",
    "        #print res['statuses'][j]['id']\n",
    "        if res['statuses'][j]['id'] < minid:\n",
    "            minid = res['statuses'][j]['id']-1\n",
    "    print 'New minimum ID:', minid\n",
    "    allwomen.extend(res['statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmen = []\n",
    "term = '%22men+can+be%22'\n",
    "minid = 9999999999999999999999\n",
    "for i in range(0,30):\n",
    "    print 'Up to tweet', minid, 'iteration', i\n",
    "    res = tsearch.search.tweets(q=term,count=100,result_type='recent',max_id=minid)\n",
    "    print 'Hits:', res['search_metadata']['count']\n",
    "    if res['search_metadata']['count']==0:\n",
    "        break\n",
    "    #minid = re.search('max_id=([^&]+)&',res['search_metadata']['next_results']).group(1)\n",
    "    for j in range(0,len(res['statuses'])):\n",
    "        #print res['statuses'][j]['id']\n",
    "        if res['statuses'][j]['id'] < minid:\n",
    "            minid = res['statuses'][j]['id']-1\n",
    "    print 'New minimum ID:', minid\n",
    "    allmen.extend(res['statuses'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we look for here?\n",
    "\n",
    "We could do more sophisticated analysis, and would probably want to if we were collecting this data for an actual project.  But let's just look at a really simple question: what adjectives are ascribed to men and women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractconcordance(t,r):\n",
    "    tweettext = t['text']\n",
    "    nextwordhit = re.search(r,tweettext)\n",
    "    if nextwordhit is None:\n",
    "        return 'No match'\n",
    "    else:\n",
    "        return nextwordhit.group(1)\n",
    "\n",
    "womenre = re.compile('women can be ([^ ]+)',re.I)\n",
    "\n",
    "#for t in allwomen[0:15]:\n",
    "#    print t['text']\n",
    "#    print extractconcordance(t,womenre)\n",
    "\n",
    "womendict = {}\n",
    "for t in allwomen:\n",
    "    word = extractconcordance(t,womenre).lower()\n",
    "    womendict[word] = womendict.get(word,0)+1\n",
    "\n",
    "pprint(womendict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(np.sort(np.array([(k,v) for (k,v) in womendict.iteritems()],dtype=[('word', '<U43'), ('count', 'i8')]),axis=0,order=['count'])[-50:])\n",
    "pprint({k: v for [k,v] in womendict.iteritems() if v>4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menre = re.compile('men can be ([^ ]+)',re.I)\n",
    "\n",
    "#for t in allwomen[0:15]:\n",
    "#    print t['text']\n",
    "#    print extractconcordance(t,womenre)\n",
    "\n",
    "mendict = {}\n",
    "for t in allmen:\n",
    "    word = extractconcordance(t,menre).lower()\n",
    "    mendict[word] = mendict.get(word,0)+1\n",
    "\n",
    "pprint(mendict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint({k: v for [k,v] in mendict.iteritems() if v>4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions, etc.\n",
    "\n",
    "So sure enough, we see stereotypical language being used, even in this really simple analysis.  How could we expand the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A third example, if you're interested\n",
    "\n",
    "Here's a problem I've been working on lately: the relationship between actual and prescribed gendered and gender-neutral language use.\n",
    "\n",
    "Many style guides still claim that *everyone put on their coats* is bad, and *everyone put on **his** coat* is the only acceptable form. Twitter might not have high editorial standards, but it's an interesting case for looking at how people use their language, and it might help us understand if *they* sounds natural.  So let's compile some data!\n",
    "\n",
    "Specifically, let's start out by comparing two searches:\n",
    "* *everybody their*\n",
    "* *everybody his*\n",
    "\n",
    "What I'd like to do is get at two important ratios:\n",
    "* what are the relative frequencies of these options? (tweets/day)\n",
    "* what proportion of each of these is a case where *their/his* is referring to *everybody*?\n",
    "\n",
    "And to follow it up, I'd like to compare two more alternatives:\n",
    "* *everybody \"his or her\"*\n",
    "* *everybody her*\n",
    "\n",
    "*Everybody had his or her coat on*, or things in that vein, are often offered as compromises, and many of us take that compromise in our writings. But, as your Twitter data (probably) shows, this is a formal circumvention, and almost everyone naturally uses *their* in this situation.  As it turns out, we've been using gender-neutral *they* in these contexts for centuries, in fact! (Bodine 1975)\n",
    "\n",
    "Anyway, this is one way of seeing the formal/informal distinction in language use between an edited corpus (like Google Books) and a more conversational corpus (like Twitter). Twitter gets us into new territories, and we'll see that even more in Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To go back 3000 tweets, you need 30 searches at the default count of 100 tweets per search. \n",
    "#Find out if you have enough searches left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = tsearch.application.rate_limit_status()\n",
    "#remaining = r['resources']['search']['/search/tweets']['remaining']\n",
    "#print remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct an *everybody their* search, and iterate through it 30 times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
