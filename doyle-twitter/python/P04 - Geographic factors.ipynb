{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One last thing: Twitter geography\n",
    "\n",
    "I don't know how much of this we're going to be able to cover, so it's going to be a little sparse.  One of my major interests in Twitter corpora is the geographic aspect of tweets. Tweets *can* contain geographic information, though many don't.  This information can come from multiple sources: the tweet itself can be **geotagged**, with a latitude-longitude pair or city name, or the user can have a self-specified **location** (which is much noisier and may be a stright-up lie).  Around 3% of tweets have geotags, and in my experience, somewhere around 30-60% have usable geographic information in the user's location (this depends on what granularity of geographic information you're willing to accept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The usual initialization stuff\n",
    "\n",
    "import sys, os, re\n",
    "from pprint import pprint                           #Important for reading through JSONs\n",
    "from time import localtime,strftime,sleep,time      #Important for dealing with Twitter rate limits\n",
    "import datetime                       #Important for processing Twitter timestamps\n",
    "import twitter\n",
    "\n",
    "cons_oauth_file = 'c.xxx'\n",
    "if os.path.exists(cons_oauth_file):\n",
    "    constoken, conssecret = twitter.read_token_file(cons_oauth_file)\n",
    "else:\n",
    "    constoken = raw_input(\"What is your app's 'Consumer Key'?\").strip()\n",
    "    conssecret = raw_input(\"What is your app's 'Consumer Secret'?\").strip()\n",
    "    wf = open(cons_oauth_file,'w'); wf.write(constoken+'\\n'+conssecret); wf.close()\n",
    "    \n",
    "app_oauth_file = 'a.xxx'\n",
    "if not os.path.exists(app_oauth_file):\t\t\t\t\t\t\t\t\t#if user not authorized already\n",
    "\ttwitter.oauth_dance(\"your app\",constoken,conssecret,app_oauth_file)\t\t#perform OAuth Dance\n",
    "apptoken, appsecret = twitter.read_token_file(app_oauth_file)\t\t\t\t\t#import user credentials\n",
    "\n",
    "tsearch = twitter.Twitter(auth=twitter.OAuth(apptoken,appsecret,constoken,conssecret))\t#create search command\\\n",
    "\n",
    "def extracttweetURL(j):\n",
    "\treturn 'http://twitter.com/'+j['user']['screen_name']+'/status/'+str(j['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's do a really simple search, and one that has a well-known geographic distribution.  Specifically, let's look for tweets mentioning interstates.\n",
    "\n",
    "I-85 is an interstate in the Southeastern U.S., running from Alabama to Virginia.  [Wikipedia](https://en.wikipedia.org/wiki/Interstate_85) has a nice little map of the route.  Let's see if we can't re-create that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term1 = '\"I-85\"'\n",
    "term1 = re.sub(' ','+',term1)\n",
    "term = re.sub('\\\"','%22',term1)\n",
    "\n",
    "print \"Searching for\", term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"lib\"))\n",
    "\n",
    "from seetweetlib225 import *\n",
    "from pprint import pprint\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global variables\n",
    "loclist = [\"30.8,-98.6\",\"39.8,-95.6\",\"32.8,-117.6\",\"37.8,-122.6\"]\n",
    "radius = \"2500km\"\n",
    "multiloc = True\n",
    "tweetspersearch = 75\n",
    "maxpages = 20\n",
    "maxid = float(\"+inf\")\n",
    "importcsv = ''\n",
    "overwrite = 'w'\n",
    "outfile = ''\n",
    "keeptweets = True\n",
    "startat = 0\n",
    "header = True\n",
    "throttle = True\n",
    "checklimits = 3 \t\t#To avoid overquerying the rate_limit function, only check every N iterations\n",
    "importmultiloc = False\n",
    "newmultiloc = False\n",
    "onlyincltweets = True\n",
    "wff = False\n",
    "trackfails = False\n",
    "MAXERRORS = 3\n",
    "scheduled = False\n",
    "raw = True\n",
    "tweetcount = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your account to use SeeTweet\n",
    "\n",
    "First things first, you need a Twitter account through which to access the REST API. Just in case you run afoul of any Twitter regulations---and it's not inconceivable as you start out---I recommend using an account you could survive being Twitter jailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsearch = authorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a search term\n",
    "\n",
    "Okay, we're going to look for some term here. You can look for individual words or a phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term1 = '\"tractor trailer\"'\n",
    "term1 = re.sub(' ','+',term1)\n",
    "term = re.sub('\\\"','%22',term1)\n",
    "\n",
    "print \"Searching for\", term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code\n",
    "\n",
    "This will be revised to simply & improve transparency. Ideally will be reduced to simple API calls with reduced opaque processing.  Some additional processing I'm currently doing will also be removed.\n",
    "\n",
    "Good feature to add here is URL for each discovered tweet in case you want to look at what's going on as it comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Basic startup processing\n",
    "if (importcsv and multiloc):\n",
    "\timportmultiloc = True\n",
    "elif multiloc:\n",
    "\tnewmultiloc = True\n",
    "\n",
    "\n",
    "if importcsv:\n",
    "\trf = open(importcsv,'r')\n",
    "\tfirstline=True\n",
    "\ttidnum = 0\n",
    "\ttids = []\n",
    "\tcenters = []\n",
    "\tincls = []\n",
    "\tfor line in rf:\n",
    "\t\tif line[0] == '#':\n",
    "\t\t\tcontinue\n",
    "\t\tsplitline = line.strip().split(',')\n",
    "\t\tif firstline:\n",
    "\t\t\ttidnum = splitline.index('tid')\n",
    "\t\t\tif importmultiloc:\n",
    "\t\t\t\tcenternum = splitline.index('center')\n",
    "\t\t\t\tinclnum = splitline.index('incl')\n",
    "\t\t\tfirstline=False\n",
    "\t\telse:\n",
    "\t\t\tif onlyincltweets:\t\t\t\t\t\t\t#If we're excluding excluded tweets from baseline calc, skip to next line if incl=0\n",
    "\t\t\t\tif int(splitline[inclnum])==0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\ttids.append(long(splitline[tidnum])-1)\n",
    "\t\t\tif importmultiloc:\n",
    "\t\t\t\tcenters.append(int(splitline[centernum]))\n",
    "\t\t\t\tincls.append(int(splitline[inclnum]))\n",
    "\ttids = tids[startat:]\n",
    "\tif importmultiloc:\n",
    "\t\tcenters = centers[startat:]\n",
    "\t\tincls = incls[startat:]\n",
    "\tmaxpages = len(tids)\n",
    "\ttids.append(0)\n",
    "\tfirsthit = tids[0]\n",
    "\trf.close()\n",
    "else:\n",
    "\tfirsthit = float(\"+inf\")\n",
    "\n",
    "outcomes = {}\n",
    "locnum = -1\n",
    "if newmultiloc:\n",
    "\ttweetlist = [0]*len(loclist)\n",
    "\tsearchesleft = [0]*len(loclist)\n",
    "\tmintids = [0]*len(loclist)\n",
    "\tmaxtids = [0]*len(loclist)\n",
    "elif importmultiloc:\n",
    "\ttweetlist = [0]*maxpages\t\t#tweetlist[tweetnum][locnum][resnum] = [outline,loc,tid] for the resnum-th baseline tweet in loc locnum on testtweet tweetnum\n",
    "\tmintids = [0]*maxpages\n",
    "\tmaxtids = [0]*maxpages\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual search-performing code\n",
    "outfile = ''\n",
    "\n",
    "if importcsv:\n",
    "\tif not outfile:\n",
    "\t\toutfile = 'base.'+term1.strip('\\\"')+'.'+importcsv\n",
    "else:\n",
    "\t#if scheduled:\n",
    "\t#\tscheddir = 'scheduled'\n",
    "\t#\tif not os.path.exists(scheddir):\n",
    "\t#\t\tos.mkdir(scheddir)\n",
    "\t#\toutfile = scheddir+'/'+term1.strip('\\\"')+'.'+strftime('%Y%m%d.%H%M')+'.csv'\n",
    "\tif not outfile:\n",
    "\t\toutfile = term1.strip('\\\"')+'.csv'\n",
    "wf = open(outfile,overwrite)\n",
    "latlong = loclist[0]\n",
    "#TODO: block out the header stuff\n",
    "if header:\n",
    "\twf.write('#Compiled by SeeTweet '+versionnum+'.\\n')\n",
    "\twf.write('#Search performed at '+strftime('%Y-%m-%d %H:%M')+'\\n')\n",
    "\tif not multiloc:\n",
    "\t\twf.write('#Search location: '+latlong+','+radius+'\\n')\n",
    "\telse:\n",
    "\t\twf.write('#Search location: U.S. 4-location coverage points (Texas, KC, SD, SF)\\n')\n",
    "\twf.write('#Search term: '+term1+'\\n')\n",
    "if (overwrite=='w' and not multiloc):\n",
    "\twf.write('day,year,month,date,hour,minute,second,source,city,state,lat,long,uid,tid\\n')\n",
    "elif (overwrite=='w' and newmultiloc):\n",
    "\twf.write('day,year,month,date,hour,minute,second,source,city,state,lat,long,uid,tid,center,incl\\n')\n",
    "elif (overwrite=='w' and importmultiloc):\n",
    "\twf.write('day,year,month,date,hour,minute,second,source,city,state,lat,long,uid,tid,origtid,origincl,center,incl\\n')\n",
    "if keeptweets:\n",
    "\ttweetdir = 'tweetarchive'\n",
    "\tif not os.path.exists(tweetdir):\n",
    "\t\tos.mkdir(tweetdir)\n",
    "\t#if not os.path.exists(tweetdir+'/'+scheddir):\n",
    "\t#\tos.mkdir(tweetdir+'/'+scheddir)\n",
    "\touttweetfile = tweetdir+'/'+os.path.splitext(outfile)[0]+'.tweets'\n",
    "\twft = open(outtweetfile,overwrite)\n",
    "\tif overwrite=='w':\n",
    "\t\tif raw:\n",
    "\t\t\twft.write('day\\tyear\\tmonth\\tdate\\thour\\tminute\\tsecond\\tloc\\tuid\\ttid\\ttweet\\ttlength\\trt\\n')\n",
    "\t\telse:\n",
    "\t\t\twft.write('loc,uid,tid,tweet\\n')\n",
    "if trackfails:\n",
    "\tfaildir = 'failures'\n",
    "\tif not os.path.exists(faildir):\n",
    "\t\tos.mkdir(faildir)\n",
    "\toutfailfile = faildir+'/'+os.path.splitext(outfile)[0]+'.fails'\n",
    "\twff = open(outfailfile,'w')\n",
    "\t\n",
    "\t\n",
    "\t#Note: block out the rate check\n",
    "for latlong in loclist:\n",
    "\tlocnum = locnum + 1\n",
    "\tgeocodestr = latlong+\",\"+radius\n",
    "\tprint \"\\nSearch centered at:\", latlong, \"(locnum \"+str(locnum)+\")\"\n",
    "\tif newmultiloc:\n",
    "\t\tcurrloctweets = []\n",
    "\t\ttidbycurrloc = []\n",
    "\t\n",
    "\tfor pagenum in range(0,maxpages):\n",
    "\t\tprint ''\n",
    "\t\tif importmultiloc:\n",
    "\t\t\tif (locnum==0):\n",
    "\t\t\t\ttweetlist[pagenum] = []\n",
    "\t\t\t\tmaxtids[pagenum] = []\n",
    "\t\t\t\tmintids[pagenum] = []\n",
    "\t\t\t#print maxtids\n",
    "\t\t\tcurrloctweets = []\n",
    "\t\t\ttidbycurrloc = []\n",
    "\t\t#Examining the rate limit\n",
    "\t\tif (pagenum % checklimits == 0):\n",
    "\t\t\tr = getlimits(tsearch)\n",
    "\t\t\tif r['remaining'] <= checklimits:\n",
    "\t\t\t\tprint \"\\n\\n**Paused because of rate limit.**\"\n",
    "\t\t\t\tprint \"Current time:\",strftime('%I:%M:%S')\n",
    "\t\t\t\tprint \"Reset time:  \",strftime('%I:%M:%S',localtime(r['reset']))\n",
    "\t\t\t\tif not multiloc:\n",
    "\t\t\t\t\tprint \"Resume with flag -s=\"+str(startat+pagenum)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint \"Stopped on location \"+str(locnum)+\", tweet \"+str(startat+pagenum)+\"/\"+str(maxpages)\n",
    "\t\t\t\tprint \"--\"\n",
    "\t\t\t\tprint tweetcount, 'tweets found. Centered at', geocodestr\n",
    "\t\t\t\tprint outcomes\n",
    "\t\t\t\tif throttle:\n",
    "\t\t\t\t\twaittime = r['reset']-time()+30\n",
    "\t\t\t\t\tprint \"Waiting\", round(waittime), \"seconds before resuming.\"\n",
    "\t\t\t\t\tsleep(waittime)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsys.exit()\n",
    "\t\t\telif r['remaining'] < 11:\n",
    "\t\t\t\tprint \"\\n\\n**WARNING:\", r['remaining'], \"queries remaining.**\"\n",
    "\t\t\t\tprint \"Current time:\",strftime('%I:%M:%S')\n",
    "\t\t\t\tprint \"Reset time:  \",strftime('%I:%M:%S',localtime(r['reset']))\n",
    "\t\t\t\tprint \"\"\n",
    "\t\t\t\tprint \"\\n\\n\"\n",
    "\t\t\t\tsleep(10)\n",
    "\t\t\n",
    "\t\t#Adding a catch for various Twitter errors\n",
    "\t\terrors = 0\n",
    "\t\twhile (errors < MAXERRORS):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif (firsthit == float(\"+inf\")):\n",
    "\t\t\t\t\tres = tsearch.search.tweets(q=term+'+-rt',geocode=geocodestr,count=str(tweetspersearch),result_type=\"recent\")\n",
    "\t\t\t\t\t#pprint(res['statuses'][99])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t#print 'Test:', term+'+-rt',geocodestr,str(tweetspersearch),str(firsthit)\n",
    "\t\t\t\t\tres = tsearch.search.tweets(q=term+'+-rt',geocode=geocodestr,count=str(tweetspersearch),result_type=\"recent\",max_id=str(firsthit))\n",
    "\t\t\t\t\t#pprint(res['statuses'][99])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept TwitterHTTPError as e:\n",
    "\t\t\t\terrors = errors + 1\n",
    "\t\t\t\tprint \"Twitter Error encountered. Retrying\",MAXERRORS-errors,\"more times.\"                                                       \n",
    "\t\t\t\tprint \"\\n\"+e.response_data\n",
    "\t\t\t\tsleep(5)\n",
    "\t\tif (errors==MAXERRORS):\n",
    "\t\t\tprint \"Repeated errors encountered, possibly due to rate limit.\"\n",
    "\t\t\tprint \"Will wait 15 minutes and try once more before quitting.\"\n",
    "\t\t\tsleep(900)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif (firsthit == float(\"+inf\")):\n",
    "\t\t\t\t\tres = tsearch.search.tweets(q=term+'+-rt',geocode=geocodestr,count=str(tweetspersearch),result_type=\"recent\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tres = tsearch.search.tweets(q=term+'+-rt',geocode=geocodestr,count=str(tweetspersearch),result_type=\"recent\",max_id=str(firsthit))\n",
    "\t\t\texcept:\t\t\t\n",
    "\t\t\t\traise Exception(\"Gave up because of repeated errors, sorry.\")\n",
    "\t\tres = res['statuses']\n",
    "\t\t\t\n",
    "\t\tprint len(res), 'hits on page', startat+pagenum+1, '(max_id='+str(firsthit)+')'\n",
    "\t\tprint r['remaining']-1, 'queries remaining.'\n",
    "\t\tif (len(res)==0):\n",
    "\t\t\tprint 'Out of tweets at this location.'\n",
    "\t\t\t#print 'Test:', term+'+-rt',geocodestr,str(tweetspersearch),str(firsthit)\n",
    "\t\t\t#sleep(1)\n",
    "\t\t\t#res = tsearch.search.tweets(q=term+'+-rt',geocode=geocodestr,count=str(tweetspersearch),result_type=\"recent\",max_id=str(firsthit))\n",
    "\t\t\t#pprint(res['statuses'][99])\n",
    "\t\t\tbreak\n",
    "\t\tfor i in range(0,len(res)):\n",
    "\t\t\ttweetcount = tweetcount + 1\n",
    "\t\t\t[outline,outcome,tid,tline] = extractinfo(res[i],wff,raw)\n",
    "\t\t\tif outline:\n",
    "\t\t\t\tif not multiloc:\n",
    "\t\t\t\t\twf.write(outline)\n",
    "\t\t\t\telif newmultiloc:\n",
    "\t\t\t\t\tcurrloctweets.append([outline[:-1]+','+str(locnum)+'\\n',locnum,long(tid)])\n",
    "\t\t\t\t\ttidbycurrloc.append(float(tid))\n",
    "\t\t\t\t\t#wf.write(outline[:-1]+','+str(locnum)+'\\n')\n",
    "\t\t\t\telif importmultiloc:\n",
    "\t\t\t\t\tcurrloctweets.append([outline[:-1]+','+str(tids[pagenum]+1)+','+str(incls[pagenum])+','+str(locnum)+'\\n',locnum,long(tid)])\n",
    "\t\t\t\t\ttidbycurrloc.append(float(tid))\n",
    "\t\t\tif keeptweets:\n",
    "\t\t\t\t#wft.write(tline.encode('ascii','ignore'))\n",
    "\t\t\t\tif not raw:\n",
    "\t\t\t\t\twft.write(tline.encode('ascii','ignore'))\n",
    "\t\t\t\tif raw:\n",
    "\t\t\t\t\twft.write(tline.encode('ascii','replace'))\t\t#maybe create a separate .utweets file with the Unicode versions of tweets\n",
    "\t\t\toutcomes[outcome] = outcomes.get(outcome,0)+1\n",
    "\t\t\tif importcsv:\n",
    "\t\t\t\tfirsthit = tids[pagenum+1]\n",
    "\t\t\telse:\n",
    "\t\t\t\tif firsthit > long(tid):\t\t\t#if current tweet came before previous oldest, update oldest\n",
    "\t\t\t\t\tfirsthit = long(tid)-1\n",
    "\t\tif importmultiloc:\n",
    "\t\t\tif len(tidbycurrloc) > 0:\n",
    "\t\t\t\tmaxtids[pagenum].append(max(tidbycurrloc))\n",
    "\t\t\t\tmintids[pagenum].append(min(tidbycurrloc))\n",
    "\t\t\telse:\n",
    "\t\t\t\tmaxtids[pagenum].append(0)\n",
    "\t\t\t\tmintids[pagenum].append(0)\t\t\t\t\n",
    "\t\t\ttweetlist[pagenum].append(currloctweets)\n",
    "\t#endfor searches within a location\n",
    "\tif newmultiloc:\n",
    "\t\tif len(tidbycurrloc) > 0:\n",
    "\t\t\tmaxtids[locnum] = max(tidbycurrloc)\n",
    "\t\t\tmintids[locnum] = min(tidbycurrloc)\n",
    "\t\telse:\n",
    "\t\t\tmaxtids[locnum] = 0\n",
    "\t\t\tmintids[locnum] = 0\n",
    "\t\tsearchesleft[locnum] = maxpages-pagenum-1\t\t\t#calculating how many pages were left in the most maxed-out search\n",
    "\t\ttweetlist[locnum] = currloctweets\n",
    "\toverwrite = 'a+'\n",
    "\theader = False\n",
    "\tif importcsv:\n",
    "\t\tfirsthit = tids[0]\n",
    "\telse:\n",
    "\t\tfirsthit = float(\"+inf\")\n",
    "\n",
    "#Endfor multiple locations\n",
    "if newmultiloc:\n",
    "\tbalanceandprint(tweetlist,mintids,maxtids,searchesleft,wf)\n",
    "elif importmultiloc:\n",
    "\tfor pagenum in range(0,maxpages):\n",
    "\t\tbalanceandprint(tweetlist[pagenum],mintids[pagenum],maxtids[pagenum],[0]*len(loclist),wf)\n",
    "wf.close()\n",
    "if keeptweets:\n",
    "\twft.close()\n",
    "if trackfails:\n",
    "\twff.close()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ewf = open('I-85.csv','r')\n",
    "ewlist = []\n",
    "header = 0\n",
    "for line in ewf:\n",
    "    if header<5:\n",
    "        header += 1\n",
    "        continue\n",
    "    splitline = line.strip().split(',')\n",
    "    if splitline[10]!='NA':\n",
    "        ewlist.append([splitline[10],splitline[11]])\n",
    "ewf.close()\n",
    "iarr = np.array(ewlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ewf = open('eighteen-wheeler.csv','r')\n",
    "ewlist = []\n",
    "header = 0\n",
    "for line in ewf:\n",
    "    if header<5:\n",
    "        header += 1\n",
    "        continue\n",
    "    splitline = line.strip().split(',')\n",
    "    if splitline[10]!='NA':\n",
    "        ewlist.append([splitline[10],splitline[11]])\n",
    "ewf.close()\n",
    "ewarr = np.array(ewlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(ttarr[:,1], ttarr[:,0], color='b', s=2, alpha=.4)\n",
    "ax.scatter(iarr[:,1], iarr[:,0], color='r', s=2, alpha=.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ewf = open('eighteen-wheeler.csv','r')\n",
    "ewlist = []\n",
    "header = 0\n",
    "for line in ewf:\n",
    "    if header<5:\n",
    "        header += 1\n",
    "        continue\n",
    "    splitline = line.strip().split(',')\n",
    "    if splitline[10]!='NA':\n",
    "        ewlist.append([splitline[10],splitline[11]])\n",
    "ewf.close()\n",
    "ewarr = np.array(ewlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewarr[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ewf = open('tractor+trailer.csv','r')\n",
    "ewlist = []\n",
    "header = 0\n",
    "for line in ewf:\n",
    "    if header<5:\n",
    "        header += 1\n",
    "        continue\n",
    "    splitline = line.strip().split(',')\n",
    "    if splitline[10]!='NA':\n",
    "        ewlist.append([splitline[10],splitline[11]])\n",
    "ewf.close()\n",
    "ttarr = np.array(ewlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(ewarr[:,1], ewarr[:,0], color='b', s=2, alpha=.4)\n",
    "ax.scatter(ttarr[:,1], ttarr[:,0], color='b', s=2, alpha=.4)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
